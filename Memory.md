Summary
=======
Description of memory used by computer programs.

Tier 1.00: Memory is Basic
==========================
Memory is a collection of bits which can be stored and accessed. A single _bit_ can hold the value of 0 or 1. When 8 bits are stored in sequence this is called a _byte_. These collections of bits can be viewed in many different ways in which the order of the bits matters to the information which we may derive. In the same way that a series of letters are used to make up words, _bits_ are interpreted in groups which many programming languages refer to as types.

Tier 1.01: Memory as intrinsic types
====================================
Most CPU's use a set of common interpretations of _bits_ (memory) which a programing language will refer to as intrinsic types.

List of common names for intrinsic _integer_ (ie a number: +/-29) types (signed and unsigned) by size in bits:

- 8  bits - u8, s8, char, byte, unsigned char, signed char, uint8_t, int8_t, sbyte
- 16 bits - u16, s16, short, unsigned short, signed short, uint16_t, int16_t, ushort
- 32 bits - u32, s32, int, long, unsigned int, signed int, uint32_t, int32_t, uint
- 64 bits - u64, s64, long, long long, unsigned long, signed long, uint64_t, s64_t, ulong

List of common names for _floating_ _point_ (ie a number: +/-1.767) types by size in bits:
- 16 bits - half, f16
- 32 bits - float, single, f32
- 64 bits - double, f64

Notes on intrinsic types above:
* The list above is incomplete and is NOT standardized across programming languages, C++'s _long_ is not like C#'s _long_, but in most longs are 32 or 64 bits.
* Pointer-size integer types exist in many languages and match the size of the target platform. This means a program compiled for 32 bit machine and a 64 bit machine may use different integer types for the same named type. Example type names: size_t, usize, isize

When you compile a program the type will help determine the possible instructions that will be generated by the compiler. For example, if I am adding two unsigned integers (u32) and save the result as u32, my two added values, and the resulting value will always be interpreted as a non-negative number. (extra) If I were to save the result as an s32, it's possible that we can overflow and interpreted the bits incorrectly and will see a really large positive number as a negative number.

Tier 1.02: Memory as user defined types
=======================================

Not all types within a program have a set of CPU intrinsic operations directly associated with them. Many programming languages allow for the creation of user-defined types that contain many intrinsic types that logically work together to form a set of functionality. A string (collection of _integers_) is on such type and is a user-defined type which is a collection of _u8_, or _u16_ types whereby each u8 or u16's number maps to a specific character which can be looked up in ASCII or other character encodings.

**Example**: Memory interpreted as Binary, Decimal, and ASCII string.
```

  Viewer as| Binary                     | Decimal  | ASCII       |
-----------+----------------------------+----------+-------------+
         u8| 01000001                   = 65       = "A"         |
string data| 01000001 01000010 01000101 = 65 66 69 = "A" "B" "E" |
-----------+----------------------------+----------+-------------+
```

Tier 1.03: Memory Addresses/pointers
====================================
Memory is a collection of bits, and we can look at a specific location in the memory by its address value. An address does not specify a location in bits, but rather its location in bytes. An address is logically just a number that we can use to look into the memory by x number of bytes from some known start point in memory. For this reason any number, both postive and negative, may represent a valid address.

**Example**: Address Space
```
  byte 0   byte 1   byte 2   byte 3   byte 4   byte 5
 /      \ /      \ /      \ /      \ /      \ /      \
 01111110 10000001 10010001 01001110 01101101 01110101 ...
 \                                                   /
  \______________________Memory_____________________/
```

Looking at address 4 in the memory above, we see the _byte_ "01101101".
Looking at address 0 in the memory above, we see the _byte_ "01111110".

A pointer is basically an address location with a type which describes how to interpret that address's memory and move to other addresses of the same type which are next to it in memory. (See [Pointers.md](https://github.com/Manquia/Distillery/blob/master/Pointer.md) for more details)

Tier 2.00: Memory is Aligned
============================
The smallest part of memory a CPU can address is a byte. A byte is also usually the smallest part of memory a CPU can run an operation on as well.

The CPU has memory alignment requirements for some of its operations. An operation may require 4-byte or 8-byte alignment such that it can only operate on memory addresses which are a multiple of 4 or 8. (ie. 4-byte alignment would require an address like 0, 4, 8, or 12 each of which is a multiple of 4)

**Example**: Address Space and alignment
```
  byte 0   byte 1   byte 2   byte 3
 /      \ /      \ /      \ /      \
 01111110 10000001 11010011 01111100 ...
 \                                 /
  \____________Memory_____________/
```
Let's say we want to read the memory above as a u16. A CPU may require this read operation to be 2-byte aligned. This means that we would only be able to read bytes 0,1 as a u16 and bytes 2,3 as a u16. We may be unable to read byte 1,2 as a u16 as it is not 2 byte aligned!

(extra) Most programming language compilers will resolve the CPU alignment requirements silently. If a program's design goes against the CPU's alignment requirements, the compiler should handle it; but may be forced use ineffecient methods to do so.

Tier 2.01: Memory as Unions/Tagged Unions/Sum Types/Variants and pointer casting
================================================================================
Remember a programming language's type is simply an interpretation of a set number of bits in memory. A type is NOT memory, it is the interpretation we use to derive meaning from the bits of data. For this reason, we have unions/sum types/variants which allow us to view a section of memory as if it were one of many types. 

Example: memory at...
address 10 in binary is 10001001, but as u8:  137 (unsigned decimal)
address 10 in binary is 10001001, but as s8: -119 (signed decimal)
address 10 in binary is 10001001, but as used-defined flags: alive | blind | hungry (flags)

Unions are useful when some memory/information is not relavent to all states of a given user-defined type. This can reduce the memory the programmer must reason about by restricting access to the irrelevent memory while in certain states. Unions share the memory between states and may reduce the memory footprint.
 
**Example**: (extra) Let's consider guard NPC in a game. The guard has 2 states "attacking" and "guarding". While attacking we hold onto data about the fight, let's say 84 bytes of memory. While we are guarding we only need to patrol a set path, let's say 98 bytes of memory. While guarding we are never attacking and while we are attacking we are never guarding. With this in mind, we can store the "attacking" data and the "guarding" data at the same address and use a union/Sum Type/Variant to interpret the memory according to our current state. 

General characteristics of union like memory types by name:

Unions, #place:
 * Does not keep a state value
 * Does not enforce initialization of relavent data
 * Does not restrict access to non relavent data
 
Tagged Union, Sum Types, Variants:
 * keeps a state value
 * Enforce initialization of relavent data
 * Restrict access to non-relavent data using both compile-time and runtime errors

Tier 2.02: Bit Order/Endianness
===============================
Most CPU's read from memory in 1 of 2 ways:
```
Big Endian 32-bit Register |    Little Endian-32 bit Register
       +--+--+--+--+       |           +--+--+--+--+
       |F0|0E|E0|0D|       |           |F0|0E|E0|0D|
Memory +--+--+--+--+       |    Memory +--+--+--+--+
   +--+  /  /  /  /        |       +--+  |  |  |  |
a  |F0|_/  /  /  /         |    a  |0D|<-+--+--+--/
   +--+   /  /  /          |       +--+  |  |  |
a+1|0E|__/  /  /           |    a+1|E0|<-+--+--/
   +--+    /  /            |       +--+  |  |
a+2|E0|___/  /             |    a+2|0E|<-+--/
   +--+     /              |       +--+  |
a+3|0D|____/               |    a+3|F0|<-/
   +--+                    |       +--+
```
This can also be thought of as the writing most significant bits to least significant bits out of the register into memory. Little endian vs big endian differ on which side, left or right, is the most significant.

It is important to know this exists for when it comes up, but usually not that important unless you work across CPUs of different archetecure on software such as  networking, compilers, and file I/O.

Tier 3.00: Copy on write (optimization)
=======================================
The Copy on Write (COW) optimization is the idea that an object in memory may be shared by multiple references as 'if' it had been copied to multple places in memeory even if each reference is looking at the same memory. Once we attempt to write to the object, we copy it and then update all references to the newly copied object. If there are multiple references to an object that is being COW, then the new copy may be shared between the remaining references.

(extra) COW can greatly improve memory performance in object oriented programming. However, implimenting COW can be trickly and is especially challenging if your program is multi-threaded.

Tier 3.01: Memory may be constrained
====================================
Not all memory is created equal to the CPU. Memory has 3 basic operations: read, write, and execute. The memory's physical hardware or software may be constrained to only allow 0 or more of the operations above. These limitations are incredibly useful and violating them will often result in the termination of our program.

(extra) When debugging a huge program, adding memory constraints can prove helpful to find the source of the heap or stack corruption. (see @TODO Memory Constraints, StackSentinal.txt)


Tier 3.02: Memory speed heirarchy (General programming)
=======================================================
This is a basic diagram of the general performance to access certain hardware containers of memory. The "|" character division indicates roughly 1 or more orders of magnitude drop in speed from the previous medium. This is a rough approximation and may be incorrect even as I write it, but the general picture of speed to access memory in an application (general programing) environment.
```
Fastest
    CPU Registers
    |
    CPU L1 Cache
    CPU L2 Cache
    CPU L3 Cache
    |
    Random Access Memory (RAM)
    |
    GPU Memory (VRAM)
    Solid State Drive (SSD)
    |
    Local Area Network (LAN)
    Hard Drives (HD)
    |
    Internet Connection
Slowest
```
Tier 3.03: Files
================
Files are chuncks of memory that normally are accessed through file descriptors. A file descriptor is the operating system's (OS)'s "pointer" like object to the filesystem's storage. Once a file is open a file descriptor also may allow you change where it points to within the file.

Common functionality in filesystems

- Open: Often takes file's path and returns a file descriptor. Open may have many other options about how to open the file, where to start reading, is the file locked to other processes, and much more depending on your OS.
- Close: Takes a file descriptor and closes it.
- Seek: Move the file descriptor's reference within the file.
- Read: Read often takes a buffer of 1-n bytes and writes to that buffer with the contents of where the file descriptor references within the file.
- Write: Write often takes a buffer of 1-n bytes and writes that buffer into the file

(extra) File Input/Output (I/O) is very slow so many filesystems allow for a blocking and non-blocking modes. In blocking mode (usually the default) the function call will wait until the operation has been completed before returning. In non-blocking mode the function may return before the operation is completed allowing our program to continue working while the file operation completes.

Tier 3.04: Memory may be mapped
===============================
Memory may be mapped/redirected in many ways between processes and even across physical storage hardware. Most modern operating systems (OS) use memory mapping for a variety of task. Three common tasks you may find are shared memory, virtual memory, memory mapped files.

Shared memory is when two or more processes (aka applications/programs) have some virtual address space which map to the same physical address in memory. This allows all processes to access this memory simultaniously, however this may require synchronization depending on the design. Shared memory is one of the fastest Inter-Process Communication (IPC) methods and is often used for high bandwitdth data transfers.

Virtual memory is the process of mapping the program's memory address space into various physical memory storage which may increase the total memory available to a program. Viable storage containers may include hard drives, network drives, USB sticks, etc... In general the choice to keep the memory in RAM is related to how often it is accessed and how full much RAM is free. Most modern operating systems use virtual memory by default which is optimal for the general user who have many programs that most of the time do nothing, but are running and have allocated memory. While these programs idle, the physical memory can be freed up for active programs to use,

(extra) The OS may provide an API to control the behavior of virtual memory:
- Reserve: Reserve a block of virtual address space to map at a later time.
- Commit: Take a reserved block of virtual address space and map it directly to memory.

(extra) Virtual memory works well until you reach the the active physical RAM limit of the system. This may causes the system to "thrash memory" spending a lot of time moving memory between physical storage while the CPU waits for its memory.

Memory mapped files is when you map a file into RAM to quickly modify/update its contents logically in the filesystem. This maps some of the application's address space onto the filesystem's address space so that any pointers into the applications address space modify the filesystem's file directly. In general this does not do file I/O on read/write, but save parts or the whole buffer when needed.

(extra) Performance consiquences: This technique can remove a copy step when saving the file to disk. It also requires significantly less system calls which may be very slow depending on your OS. It also may be very slow depending on address resolution and how file mapping behaves on the OS and system environment.

(extra) A ram drive is basically this idea, but more integrated into filesystem and drivers.

Teir 3.05: Cache
================
Caching memory creates a copy of a larger buffer/storage of memory which may be more quickly accessed than the larger buffer. Caching is a main component in performance speedups in recent years and encourages developers to use predictable memory access patterns in contigious memory.

Teir 3.06: Cache (CPU)
======================
A program's memory access paterns should be predictable because the CPU will predict and pre-copy into the cache ahead of time to reduce latency.

A program's access paterns should utilize contigious memory because the cache is populated by cachelines which are large blocks of contigious memory. If we spend the time to copy the memory from ram into cache we should try and use 100% of the cacheline to maximize throughput.

Using a small portion of a cacheline will reduce throughput and decreases the CPU's cache "health". The CPU has a limited cache size and copying new cachelines will force eviction of old cachelines which may be in use or will be used soon.

CPU cache often has levels with ever increasing size + latency
Level 1 (L1) Cache - Often every core on the CPU has a dedicated L1 cache that is not shared between cores and is small and is very fast to access.
Level 2 (L2) Cache - Often shared between 2 or more cores on the CPU, is substantially larger than L1 but slower.
Level 3 (L3) Cache - Often shared between all CPU cores and is very large, but much slower than L2 but faster than main memory.

(IMPORTANT) A program's performance is often directly tied to how well it uses the hardware's memory because memory is the slowest part on modern computers. Using preditable access patterns in contigious memory is currently the most performant way to maximize throughput for the common performance problem.

(extra) Frequently accessed memory is often refered to as "Hot" memory while "Cold" memory refers to infrequently accessed memory.

(extra) Most CPUs have a separate instruction cache (I-cache). The I-cache has a limited size and some hardware may incur additional cost for programming features such as exceptions or inlining of "cold" functions because the set of hot instructions may no longer fits nicely into the I-cache.

(extra) Some CPUs have special memory streaming instructions which will allow basic moving and maping operations on memory without affecting all or any of the CPU's cache levels. These instructions are commonly used in graphics, audio, and OS level operations which want to avoid negativly affecting the CPU cache health.

(extra) Because of cache it is generally expensive to synchronize data across theads/cores (See Control.txt Teir 4 for more details)

Teir 3.07: Cache (File Systems/Drivers)
=======================================
When reading from a file system it is extremely ineffective to only read a few bytes at a time so the file system and drivers will read large segments of a file and save it into a buffer/cache in ram for later use.

Depending on the size of the cache this can mean that reading from different parts of a file can be extremely expensive in comparison to reading linearly from start to finish. Additionally, using Seek() to move around a file is not often predictable and may cause the file's cache to be evited to read from the new location.

Depending on your OS, you may be able to provide the memory used for caching in the file system. Providing the cache often happens while mounting the filesystem for your application.

Teir 3.08: Cache (Network/Internet)
===================================
A common way to reduce internet/network trafic and latency is to cache requests and responses and intercept duplicate requests while the response has not expired. For example, if our website updates once every minute with new content, I can post in the responce message that this page will expire every minute but is valid for much longer. The same is true for content on pages. This is one of the ways in which browsers on your computer end up using gigabytes of space even while showing relativly small amount of content.

Tier 4.00: Static Library
=========================
A static library is a file containing programing logic that may be linked into a executable or another library at compile-time.

Tier 4.01: DLL and Shared Library
=================================

A Dynamic Link Library (DLL) or Shared Library (SO) is a file containing programming logic that may be linked to a running program. A program may load zero or more functions, execute zero or more functions, and unload zero or more functions at runtime from a DLL or SO depending on its needs.

Some implimentations of DLL/SO and Operating systems may allow for programming logic sharing between processes which may improve I-cache effeciency.

Tier 4.02: Memory and your OS
=============================

Unless you are programming at the kernel level, you have an operating system (OS) that is managing the system's memory. The OS manages memory using pages, the OS has a page table for each active program. A page often maps to 4096 bytes of memory and will have its own privalges, modes, ownership, rules, et cetera depending on the OS and hardware. Management of these pages allow for the following features

Memory Modes: Allowing operations or restrictions on a memory page to read, write, or execution.

Isolated memory and shared memory: Two programs running on the same machine share physical memory but have different page tables which prevent access into eachother's memory without explicit sharing.

Copy On Write and fork:

Copy on write is a memory optimization which allows two or more programs to share pages of memory in physical hardware so long as they only read. If a program sharing this memory attempts to write to a shared page, at that time the OS will copy the page's content to a new page in memory and modify the page table accordingly. This copy process is hidden from the program but may fail in a constrained system. For programs which share a lot of read only memory (ROM), this can greatly improve effeciency and performance.

Copy on write is often initiated by a "fork" operation which clones a program.



# Incomplete

#4/5
@Runtime code modification. I might not include this since CPUs aren't performant on this type of operation.
@Runtime Inheritance (class/Object bundle, Java Script)
@General Solution for Language binding 
